---
permalink: "posts/your-eyes-they-turn-me-8eb3037abd80/"
title: Your eyes. They turn me
description: Ramblings on AI Vision and how our visual data with known intent is valuable.
date: 2016-09-18
tags:
  - technology
---

<article aria-labelledby="post-title" class="h-entry">
  <div class="section-content">
    <div class="section-inner sectionLayout--insetColumn">
      <p name="09f9" id="09f9" class="graf graf--p graf-after--h3">
        Ramblings on AI Vision and how our visual data with known intent is
        valuable.
      </p>
      <p name="f290" id="f290" class="graf graf--p graf-after--p">
        We need massive amounts of data to do mundane things, things we humans
        find easy can be trained eventually but require this information to be
        produced somehow.
      </p>
      <blockquote
        name="53d4"
        id="53d4"
        class="graf graf--blockquote graf-after--p">
        High-level reasoning requires very little computation, but low-level
        sensorimotor skills require enormous computational resources.
        <a
          href="https://en.wikipedia.org/wiki/Moravec%27s_paradox"
          data-href="https://en.wikipedia.org/wiki/Moravec%27s_paradox"
          class="markup--anchor markup--blockquote-anchor"
          rel="noopener"
          target="_blank">
          Moravec’s paradox
        </a>
      </blockquote>
      <p name="e7b3" id="e7b3" class="graf graf--p graf-after--blockquote">
        In the very near future, the data hungry algorithms will need us to take
        part at a mass scale. Businesses will reorganise around collecting this
        data, some will be set up to base their existence on gathering and
        either open sourcing this data or selling it to the highest bidder.
      </p>
      <p name="1b5c" id="1b5c" class="graf graf--p graf-after--p">
        We are seeing just the start with Google’s release of a collection of
        <strong class="markup--strong markup--p-strong">
          six hundred and fifty thousand
        </strong>
        grasp attempts by a robotic arm to grab everyday objects
        <a
          href="https://plus.google.com/+VincentVanhoucke/posts/8T7DSJhGY3u"
          data-href="https://plus.google.com/+VincentVanhoucke/posts/8T7DSJhGY3u"
          class="markup--anchor markup--p-anchor"
          rel="noopener"
          target="_blank">
          Source
        </a>
        . Tesla’s cars come with all the sensors needed to outwork a
        self-driving car (essentially, the newer models will come with some
        upgrades). These sensors are constantly watching what the drivers do (on
        the outside) and adding this data with what the driver does (steering,
        braking, etc) and where the driver wants to go (the built in maps / GPS
        system). Tesla calls this fleet learning and will then be able to
        collate an incredible collection of ‘task’ and ‘actions’ aligned with
        the ‘environment’.
        <a
          href="https://www.tesla.com/blog/upgrading-autopilot-seeing-world-radar"
          data-href="https://www.tesla.com/blog/upgrading-autopilot-seeing-world-radar"
          class="markup--anchor markup--p-anchor"
          rel="noopener"
          target="_blank">
          Source
        </a>
        .
      </p>
      <p name="6f2d" id="6f2d" class="graf graf--p graf-after--p">
        One of my favourite quotes from Andrew Ng presents a clear picture of
        why this matters.
      </p>
      <blockquote
        name="26a1"
        id="26a1"
        class="graf graf--blockquote graf-after--p">
        I think AI is akin to building a rocket ship. You need a huge engine and
        a lot of fuel. If you have a large engine and a tiny amount of fuel, you
        won’t make it to orbit. If you have a tiny engine and a ton of fuel, you
        can’t even lift off. To build a rocket you need a huge engine and a lot
        of fuel.
      </blockquote>
      <blockquote
        name="eb16"
        id="eb16"
        class="graf graf--blockquote graf-after--blockquote">
        The analogy to deep learning is that the rocket engine is the deep
        learning models and the fuel is the huge amounts of data we can feed to
        these algorithms.
        <br />
        <a
          href="https://www.wired.com/brandlab/2015/05/andrew-ng-deep-learning-mandate-humans-not-just-machines/"
          data-href="https://www.wired.com/brandlab/2015/05/andrew-ng-deep-learning-mandate-humans-not-just-machines/"
          class="markup--anchor markup--blockquote-anchor"
          rel="noopener"
          target="_blank">
          Source
        </a>
      </blockquote>
      <h4 name="7864" id="7864" class="graf graf--h4 graf-after--blockquote">
        Where do we come in?
      </h4>
      <p name="36b2" id="36b2" class="graf graf--p graf-after--h4">
        Nvidia have released a ‘Devbox’, this device can be set up with a basic
        camera and be connected to most cars reasonably easily. This is an
        end-to-end learning system that does what the Tesla cars do at a much
        more basic (aka cheaper) level. Taking a heap of data about the
        environment in raw format and aligning it with actions to eventually
        produce a computer’s visual understanding of the boundaries of the road.
      </p>
      <p name="1c32" id="1c32" class="graf graf--p graf-after--p">
        Imagine if a company wanted to get a heap of information about how to
        drive, they could collect this by putting a box like this in one
        thousand cars for a month and get about ninety thousand hours of driving
        data. This amount is paltry compared to what is probably needed though.
      </p>
      <p name="2d06" id="2d06" class="graf graf--p graf-after--p">
        We take our view of reality very much for granted, teaching computers to
        have the same view is extremely hard, but adding data to the
        mix — massive amounts of data — can help. The simple act of walking up
        to a door, knocking and handing over something is incredibly complex and
        takes a massive amount of hand eye coordination and reasoning of the
        world around us.
      </p>
      <h4 name="0613" id="0613" class="graf graf--h4 graf-after--p">
        Giving the machines vision
      </h4>
      <p name="ceee" id="ceee" class="graf graf--p graf-after--h4">
        Take the driving example a step further and it would not be unreasonable
        for a company like Dominoes to be investigating automating the delivery
        of pizzas. To do this, they need to know what every door in the world
        looks like, or at least a reasonable percentage for a good machine
        learning algorithm. One approach would be to fit each delivery driver
        with a camera, even one lens (non 3D vision) would be sufficient.
        Recording approximately five hundred thousand deliveries a day
        <a
          href="http://marketrealist.com/2015/03/dominos-pizza-serving-1-5-million-pies-day/"
          data-href="http://marketrealist.com/2015/03/dominos-pizza-serving-1-5-million-pies-day/"
          class="markup--anchor markup--p-anchor"
          rel="noopener"
          target="_blank">
          Source
        </a>
        for a few weeks should give the company a start.
      </p>
      <p name="51ae" id="51ae" class="graf graf--p graf-after--p">
        Ignoring the inevitable backlash from pizza patrons and privacy
        concerns, you end up with a treasure trove of something simple to humans
        but hard for machines. Task: Reason where the door is, go towards it and
        knock. Action: 10 million 2D recordings of direction towards a door.
      </p>
      <h4 name="47f9" id="47f9" class="graf graf--h4 graf-after--p">
        What is next
      </h4>
      <p name="d47f" id="d47f" class="graf graf--p graf-after--h4">
        This leaves us all at an interesting impasse, we want the benefits of
        more automation — it will lead to cheaper everything, but in getting
        there we will eventually replace our own actions.
      </p>
      <p name="0e53" id="0e53" class="graf graf--p graf-after--p">
        This is not new, businesses already gather incredible amounts of data,
        Google and Bing are leading the way in the areas of speech and image
        recognition because they already have the data available. The future
        shift to be aware of is when it gets distributed to a whole new level,
        when your own eyes become valuable to our next generation of machines.
      </p>
      <h4 name="e0f1" id="e0f1" class="graf graf--h4 graf-after--p">
        Closing Thoughts
      </h4>
      <p
        name="66df"
        id="66df"
        class="graf graf--p graf-after--h4 graf--trailing">
        Happy to take any comments, I am merely writing some thoughts that I
        have gathered while reading a lot about AI trends and have not
        personally written any machine learning algorithms. However, if you are
        like me and curious I recommend you watch talks by Andrew NG and
        subscribe to
        <a
          href="https://www.getrevue.co/profile/azeem"
          data-href="https://www.getrevue.co/profile/azeem"
          class="markup--anchor markup--p-anchor"
          rel="noopener"
          target="_blank">
          Exponential View
        </a>
        .
      </p>
    </div>
  </div>
  <aside>
    <p>
      Previously hosted on
      <a href="https://medium.com/@_lb_/your-eyes-they-turn-me-8eb3037abd80">
        Medium
      </a>
      exported on September 9, 2023.
    </p>
  </aside>
</article>
